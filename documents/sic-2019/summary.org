#+title: Computational Experiments on Task-Based @@latex:\\@@ Parallel Applications
#+subtitle: /Salão de Iniciação Científica UFRGS 2019/
#+options: toc:nil author:nil

#+latex_class: article
#+latex_class_options: [twocolumn, a4paper]

#+latex_header: \usepackage[margin=1in]{geometry}
#+latex_header: \usepackage{authblk}
#+latex_header: \usepackage{titling}
#+latex_header: \setlength{\droptitle}{-1.2cm}
#+latex_header: \author{Henrique Corrêa Pereira da Silva}
#+latex_header: \author{Lucas Mello Schnorr (advisor)}
#+latex_header: \affil[]{Informatics Institute\\Universidade Federal do Rio Grande do Sul}
#+latex_header: \affil[]{}
#+latex_header: \affil[]{\normalsize\texttt{\{hcpsilva, schnorr\}@inf.ufrgs.br}}

# Explain the context more clearly to someone with no background
The incessant pursuit of ever-more performance in all kinds of computing systems
led software developers inevitably into the development of parallel
applications. These applications make use of, traditionally, multiple
/homogeneous/ hardware, be it multiple /CPU/ cores or /GPU/ boards. However,
these implementations often lack the ability to conform to less homogeneous
realities, such as systems comprising of different GPUs and CPUs joined in a
/computer cluster/. Given this demand, there has been considerable effort into
the development of /middleware/ capable of effectively distributing the workload
into these hybrid architectures, taking advantage of the whole system and
providing better performance without the need of huge investments into massive
quantities of homogeneous hardware.

# This paragraph is pretty good as is. One thing that could be cleared up is the
# whole "common realities" thing. Use other words.

There are several popular parallel programming models used currently, be it on
the /HPC/ (High Performance Computing) environment or not. Some of the more
popular ones are *message passing*, *shared memory* and *task-based* (and, even
more common, any combination of the previous, which then is called a /hybrid
model/). We can also classify said models using a more ``high level'' approach,
separating them between /SPMD/ (Single Program Multiple Data) and /MPMD/
(Multiple Program Multiple Data), which can be built upon most parallel
programming models. Having that said, the implementation of any combination of
the previously cited models has been, characteristically, a very manual process,
and, consequently, very error-prone, complex and time consuming. In order to
mitigate that, most popular implementations of those models sought out to
provide a friendly programming interface to the developer through better /APIs/
(Application Programming Interfaces) or through compiler directives.

# Explain better the different kinds of models in parallel programming and
# problem decomposition. After that, provide some examples and present the
# task-based approach (fix the next paragraph after that because as is it's
# explaining that a bit).

# Introduce task-based programming more clearly
One of the efforts into making a middleware capable of such /heterogeneous/
/computation/ is called /StarPU/, which approaches the problem with the
task-based model in mind, defining tasks into a directed acyclic graph. While
the instantiation of this graph is made in a common sequential way, StarPU takes
care of the execution and distribution of tasks in the hardware, freeing the
developer from most low-level issues. Besides its programmer-friendly way of
defining the tasks that will carry out the intended computation, a graph is also
a powerful way to declare data dependencies between said tasks, as the data
declared to the API data can also be thought as a node in said graph.

# On this paragraph focus on how StarPU works, how the task declaration works,
# how it approaches data handling.

Following the intent behind the efforts of StarPU, we aim to investigate the
task-based model of designing and implementing parallel applications using
StarPU. In order to accomplish said work, the application of traditional methods
of problem decomposition will be unavoidable, as a quality implementation of a
task-based parallel program heavily relies on said planning. Also, as more
traditional approaches generally utilize tools such as /OpenMP/ and /MPI/, it's
also part of our scope of work compare those approaches to a StarPU counterpart
while utilizing heterogeneous systems.

# After that, cite the objective of the scholarship, as in it was expected to
# design and analyze the performance of applications written using said
# task-based approach. Also talk about the process of writting parallel programs
# (maybe cite PCAM?).

# Finally, talk about the results obtained so far. Previous work also maybe?
# Anyway, here it's interesting to maybe show a run example? Even though that
# implies having a finished implementation whose execution data we could
# visualize in the first place...

* Comments                                                         :noexport:
** 2019-06-11 Meeting

*Roteiro*:

- [X] Contextualização (processamento paralelo e clusters de computadores heterogêneos CPU/GPU)
- [ ] Identificação do problema (execução paralela de aplicações paralelas de maneira eficiente)
- [X] Motivação (emprego de uma abordagem que se adéqua à computação heterogênea)
- [ ] Metodologia (criação de programas paralelos voltados à tarefas - com o DAG)
- [ ] Resultados até o momento (execuções pequenas)

** 2019-06-18 Meeting

2nd par
- [ ] Remove "and task-based"
- [ ] Transform into a phrase "(and, even more common, any combination
  of the previous, which then is called a hybrid model )."
- [ ] Give examples MPI, OpenMP. Talk about hybrid (MPI+OpenMP, very
  common today)
- [ ] A problem of these APIs (the universe based on MPI and OpenMP),
  the domain decomposition is fixed and equal to the number of
  resources. Despite the fact that it can be adapted for heterogeneous
  resources, by having different sizes for the pieces of work, if
  there is dynamic load imbalances it becomes harder to obtain good
  performances.
4th par
- [ ] The objective of this work is to investigate how StarPU and its
  task-based model can be employed to code typical applications such
  as linear algebra. Once these applications are coded, we intend to
  run them in HPC platforms to evaluate the performance through common
  HPC metrics, such as makespan (execution time), load imbalance,
  occupation, and so on.
5th par
- [ ] So far, we have implemented a block vector reduction ... 
  - [ ] How this has been implemented
