#+title: Experimentos Computacionais com Aplicações Paralelas baseadas em Tarefas
#+subtitle: Salão de Iniciação Científica UFRGS 2019
#+date: June 8, 2019
#+options: toc:nil author:nil

#+latex_class: article
#+latex_class_options: [twocolumn,a4paper]
#+latex_header: \usepackage[margin=1in]{geometry}
#+latex_header: \usepackage{lipsum}
#+latex_header: \usepackage{authblk}
#+latex_header: \author[1]{Henrique Corrêa Pereira da Silva}
#+latex_header: \author[1]{Lucas Mello Schnorr}
#+latex_header: \affil[1]{Informatics Institute\\Universidade Federal do Rio Grande do Sul}
#+latex_header: \affil[ ]{}
#+latex_header: \affil[ ]{\normalsize\textup{\texttt{\{hcpsilva, schnorr\}@inf.ufrgs.br}}}

The incessant pursuit of ever-more performance in all kinds of computing systems
led software developers inevitably into the development of parallel
applications. These applications make use of, traditionally, multiple
/homogeneous/ hardware, be it multiple CPU cores or GPU boards. However, these
implementations often lack the ability to conform to more common realities, such
as systems comprising of different GPUs and/or older CPUs joined in a /computer/
/cluster/. Given this demand, there has been considerable effort into the
development of /middleware/ capable of effectively distributing the workload
into these hybrid architectures, taking advantage of the whole system and
providing better performance without the need of huge investments into
/homogeneous/ hardware.

We can divide the current parallel programming models into two broad categories:
*process interaction* and *problem decomposition*. Models that fall into the
process interaction category generally deal with more down-to-the-metal
approaches, such as shared memory, message passing or even implicit interaction
(mainly present in functional programming). On the other hand, problem
decomposition deals mainly with higher-level approaches, such as task-based
programming, where the application is designed with the necessity of splitting
the execution into threads (also called tasks). Having said that, we can also
categorize instruction and data parallelism into problem decomposition, even
though these are generally performed by the compiler and/or hardware.

One of the efforts into making a middleware capable of such /heterogeneous/
/computation/ is called /StarPU/, which approaches the problem with a task-based
model in mind, defining tasks into a directed acyclic graph. While the creation
of this graph is made in a more common sequential way, StarPU takes care of the
execution and distribution of tasks in the hardware. Besides its
programmer-friendly way of defining the threads of execution, a graph is a
powerful way to declare data dependencies between said threads, as shared data
can also be thought as a node in the graph.
