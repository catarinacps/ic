# -*- org-export-babel-evaluate: nil -*-
# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+title: Computational Experiments on @@latex:\\@@  Task-Based Parallel Applications
#+subtitle: /Salão de Iniciação Científica UFRGS 2019/
#+options: toc:nil author:nil

#+latex_class: article
#+latex_class_options: [twocolumn, a4paper]

#+latex_header: \usepackage[margin=1in]{geometry}
#+latex_header: \usepackage{authblk}
#+latex_header: \usepackage{titling}
#+latex_header: \usepackage{palatino}
#+latex_header: \setlength{\droptitle}{-1.2cm}
#+latex_header: \author{Henrique Corrêa Pereira da Silva}
#+latex_header: \author{Lucas Mello Schnorr (advisor)}
#+latex_header: \affil[]{Informatics Institute\\Universidade Federal do Rio Grande do Sul}
#+latex_header: \affil[]{\normalsize\texttt{\{hcpsilva, schnorr\}@inf.ufrgs.br}}

#+latex: \thispagestyle{empty}

# Explain the context more clearly to someone with no background
The incessant pursuit of ever-more performance in all kinds of computing systems
led software developers inevitably into the development of parallel
applications. These applications make use of, traditionally, multiple
/homogeneous/ hardware, be it multiple /CPU/ cores or /GPU/ boards. However,
these implementations often lack the ability to conform to less homogeneous
realities, such as systems comprising of different GPUs and CPUs joined in a
/computer cluster/. Given this demand, there has been considerable effort into
the development of /middleware/ capable of effectively distributing the workload
into these hybrid architectures, taking advantage of the whole system and
providing better performance without the need of huge investments into massive
quantities of homogeneous hardware.

# This paragraph is pretty good as is. One thing that could be cleared up is the
# whole "common realities" thing. Use other words.

There are several popular parallel programming models used currently, be it on
the /HPC/ (High-Performance Computing) environment or not. Some of the more
popular ones are *message passing*, *shared memory* and, even more common, any
combination of other models, which then is called a *hybrid model*. Furthermore,
the implementation of any combination of the previously cited models has been,
characteristically, a very manual process, and, consequently, very error-prone,
complex, and time-consuming. In order to mitigate that, most popular
implementations of those models, such as /OpenMPI/, /OpenMP/ and the combination
of the previous, sought out to provide a friendly programming interface to the
developer through better /APIs/ (Application Programming Interfaces) or compiler
directives. Still, a problem with those APIs is that the domain decomposition is
fixed to the number of resources. So, despite the fact that these APIs can be
adapted to more heterogeneous resources, by having those unequal pieces of work
it becomes harder to obtain desirable performance due to dynamic load
imbalances.

# We can also classify said models using a more ``high level'' approach,
# separating them between /SPMD/ (Single Program Multiple Data) and /MPMD/
# (Multiple Program Multiple Data), which can be built upon most parallel
# programming models.

# Explain better the different kinds of models in parallel programming and
# problem decomposition. After that, provide some examples and present the
# task-based approach (fix the next paragraph after that because as is it's
# explaining that a bit).

# Introduce task-based programming more clearly
One of the efforts into making a middleware capable of such /heterogeneous/
/computation/ is called /StarPU/, which approaches the problem with the
task-based model in mind, defining tasks into a directed acyclic graph. While
the instantiation of this graph is made in a common sequential way, StarPU takes
care of the execution and distribution of tasks in the hardware, freeing the
developer from most low-level issues. Besides its programmer-friendly way of
defining the tasks that will carry out the intended computation, a graph is also
a powerful way to declare data dependencies between said tasks, as the data
declared to the API data can also be thought as a node in said graph.

# On this paragraph focus on how StarPU works, how the task declaration works,
# how it approaches data handling.

# This paragraph is very convoluted in its development of sentences and in its
# natural flow.
Following the intent behind the efforts of StarPU, we aim to investigate the
task-based model of designing and implementing parallel applications using
StarPU and how StarPU can be employed to code typical applications such as
linear algebra. Once these applications are coded, we intend to run them in HPC
platforms to evaluate the performance through common HPC metrics, such as
/makespan/ (execution time), load imbalance, occupation, and others.

# After that, cite the objective of the scholarship, as in it was expected to
# design and analyze the performance of applications written using said
# task-based approach. Also talk about the process of writting parallel programs
# (maybe cite PCAM?).

Thus far, we have implemented a block vector reduction utilizing the task-based
model. This implementation uses StarPU's C API to achieve a multi-level
reduction determined by a block size parameter, which defines the size of each
block that will be reduced to a single element. That is, the program splits the
initial vectors in sub-vectors of size determined by parameter and sums the
elements in these sub-vectors into single elements, which are then the new
vector in which this split will be reapplied until we have a single element
representing the sum of all elements. Thus, through the implementation of this
application, it was shown that the StarPU runtime is a viable way to apply the
task-based parallel computing method.

# Finally, talk about the results obtained so far. Previous work also maybe?
# Anyway, here it's interesting to maybe show a run example? Even though that
# implies having a finished implementation whose execution data we could
# visualize in the first place...

* Comments                                                         :noexport:

#+begin_src emacs-lisp :exports none
(defun get-selected-text (start end)
  (interactive "r")
  (kill-new (replace-regexp-in-string
             "\n" " "
             (if (use-region-p)
                 (let ((regionp (buffer-substring start end)))
                   (replace-regexp-in-string "  " " " regionp))))))

(global-set-key (kbd "<f9>") 'get-selected-text)
#+end_src

#+begin_src emacs-lisp :tangle yes
(add-to-list 'org-latex-classes '("newlfm" "\\documentclass{newlfm}" ("\\section{%s}" . "\\section*{%s}")
                                  ("\\subsection{%s}" . "\\subsection*{%s}") ("\\subsubsection{%s}"
                                                                              . "\\subsubsection*{%s}") ("\\paragraph{%s}" . "\\paragraph*{%s}")  ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+end_src

#+RESULTS:
| newlfm  | \documentclass{newlfm}               | (\section{%s} . \section*{%s}) | (\subsection{%s} . \subsection*{%s}) | (\subsubsection{%s} . \subsubsection*{%s}) | (\paragraph{%s} . \paragraph*{%s})   | (\subparagraph{%s} . \subparagraph*{%s})   |
| newlfm  | \documentclass{newlfm}               |                                |                                      |                                            |                                      |                                            |
| beamer  | \documentclass[presentation]{beamer} | (\section{%s} . \section*{%s}) | (\subsection{%s} . \subsection*{%s}) | (\subsubsection{%s} . \subsubsection*{%s}) |                                      |                                            |
| article | \documentclass[11pt]{article}        | (\section{%s} . \section*{%s}) | (\subsection{%s} . \subsection*{%s}) | (\subsubsection{%s} . \subsubsection*{%s}) | (\paragraph{%s} . \paragraph*{%s})   | (\subparagraph{%s} . \subparagraph*{%s})   |
| report  | \documentclass[11pt]{report}         | (\part{%s} . \part*{%s})       | (\chapter{%s} . \chapter*{%s})       | (\section{%s} . \section*{%s})             | (\subsection{%s} . \subsection*{%s}) | (\subsubsection{%s} . \subsubsection*{%s}) |
| book    | \documentclass[11pt]{book}           | (\part{%s} . \part*{%s})       | (\chapter{%s} . \chapter*{%s})       | (\section{%s} . \section*{%s})             | (\subsection{%s} . \subsection*{%s}) | (\subsubsection{%s} . \subsubsection*{%s}) |

** 2019-06-11 Meeting

*Roteiro*:

- [X] Contextualização (processamento paralelo e clusters de computadores heterogêneos CPU/GPU)
- [ ] Identificação do problema (execução paralela de aplicações paralelas de maneira eficiente)
- [X] Motivação (emprego de uma abordagem que se adéqua à computação heterogênea)
- [ ] Metodologia (criação de programas paralelos voltados à tarefas - com o DAG)
- [ ] Resultados até o momento (execuções pequenas)

** 2019-06-18 Meeting

*2nd par*
- [X] Remove "and task-based
- [X] Transform into a sentence "(and, even more common, any combination of the
  previous, which then is called a hybrid model )."
- [X] Give examples MPI, OpenMP. Talk about hybrid (MPI+OpenMP, very common
  today)
- [X] A problem of these APIs (the universe based on MPI and OpenMP), the domain
  decomposition is fixed and equal to the number of resources. Despite the fact
  that it can be adapted for heterogeneous resources, by having different sizes
  for the pieces of work, if there is dynamic load imbalances it becomes harder
  to obtain good performances.

*4th par*
- [X] The objective of this work is to investigate how StarPU and its task-based
  model can be employed to code typical applications such as linear
  algebra. Once these applications are coded, we intend to run them in HPC
  platforms to evaluate the performance through common HPC metrics, such as
  makespan (execution time), load imbalance, occupation, and so on.

*5th par*
- [X] So far, we have implemented a block vector reduction ...
  - [X] How this has been implemented
