#+TITLE: Henrique's laboratory book
#+AUTHOR: Henrique Silva
#+email: hcpsilva@inf.ufrgs.br
#+INFOJS_OPT:
#+PROPERTY: session *R*
#+PROPERTY: cache yes
#+PROPERTY: results graphics
#+PROPERTY: exports both
#+PROPERTY: tangle yes

For now this is only a scratch, as I'm still reading up others Emacs configs and
setting up my [[https://github.com/hcpsilva/dotfiles][mobile]] Linux environment.

* Journal
  :PROPERTIES:
  :ATTACH_DIR: attachments
  :ATTACH_DIR_INHERIT: t
  :END:

Here I'll write my daily tasks and to-dos so I can keep track of what I'm doing.

** To-do

*Active items*:
- [ ] add more info about cluster and server-side linux
- [ ] try the starpu examples

*Finished items*:
- [X] set up the computer
- [X] set up my table
- [X] do the first experiment of schnorr/par

** Daily

Here will lie my daily thoughts and daily happenings.

*** 2019-05-02

Today the day was basically dedicated to formatting and installing my distro in
my new computer. It has a 4:3 screen, which will surely be kinda funny to work
with. Also my computer only have one analog video entry and 2 displayports, for
some reason.

Anyway, I also researched and learned a lot about =ssh= while I was trying to
get my public key into =portal.inf.ufrgs.br=. With Pablo's and Jean's help I
fixed the permissions of my home directory in the server (the =$HOME= directory
needs the 700 permission to work with =ssh=! Probably someone messed up a few
years back when they created my user).

Tomorrow I'll finish the setting-up ordeal, I hope.

*** 2019-05-03 StarPU Hello World Examples

Before trying anything with StarPU, I tried to run the first experiment in
schnorr/par, which didn't work. The job quits with the exit code =71=, to which
I've found no information online. I'm kinda tired today but next week I'll make
sure that I talk to either Nesi or Marcelo or Matheus about it.

Also I've tried installing StarPU using spack in the cluster, but there was no
StarPU package available.

On the other hand, I did create some folder in my user to organize things up and
I've also set up the ssh keys of my new computer in almost every relevant
website.

*UPDATE*: So, when I got home I continued trying things out. I've tried to
allocate some nodes to try the simplest experiment I've tried earlier and, after
playing around and learning Slurm commands, I've noticed that I cant =ssh= into
any node because my RSA key doesn't match the one in the cluster (or doesn't
exist at all there). Maybe that's the culprit for me not being able to even get
the simplest example running through =sbatch=? I'll contact Schnorr about this.

**** StarPU "Hello World"                                             :LUCAS:
***** Install preliminary software
****** spack

See https://github.com/spack/spack to do:

#+begin_src shell :results output
git clone https://github.com/spack/spack.git
source spack/share/spack/setup-env.sh
spack find
#+end_src

Then, add the =solverstack= from the INRIA GitLab:

#+BEGIN_SRC shell :tangle yes
  git clone https://gitlab.inria.fr/solverstack/spack-repo.git solverstack
  spack repo add solverstack/
#+END_SRC

****** starpu with spack

#+begin_src shell :results output
spack info starpu
#+end_src

Verify options, then:

#+begin_src shell :results output
spack install starpu@master~cuda~examples~fast+fortran+fxt+mlr~mpi~nmad~opencl~openmp+poti+shared~simgrid~simgridmc~verbose
#+end_src

This might take some time, do it in the cluster.

Confirm the location of where starpu has been installed

#+begin_src shell :results output
spack location -i starpu
#+end_src

***** StarPU client code of two examples

There are two examples:
- ~programa.c~ (simple one-task hello world)
- ~vector_scal.c~ (multiply a vector by a scalar in parallel)

See contents in [[./experiments/starpu/hello-world/]].

Please note that we are using CMake to find the StarPU libraries.

The, do the following steps (try to understand each one).

Make sure you have ~spack~ in your ~PATH~ variable before going forward.

#+begin_src shell :results output
cd src/starpu-hello-world
mkdir -p build
cd build
cmake -DSTARPU_DIR=$(spack location -i starpu) ..
make
#+end_src

You'll have two binaries: ~programa~ and ~vector_scal~.

Verify that they have the correct libraries linked with ~ldd~.

Run both by launching these binaries in your CLI.

*** 2019-05-06

Today I ran the =hello.slurm= file from the first experiment of schnorr/par. I
had to do some modifications to the script so that it would actually find the
executable (as it wasn't finding inside the folder I was running =sbatch= from,
even though it had no trouble compiling it).

Also I've added info about MPI in the External Resources section, which are
really just some tutorials and introductions to the matter. I found the MPI
interface to be rather cumbersome with its C-like functions and inits. Doesn't
a proper C++ wrapper exist somewhere? Maybe that takes away part of the
complexity of the syntax choices. I'll look around.

Also, I'm kinda becoming really attached to my Emacs development environment.
I've gathered quite a few nice =.org= configs and I'm making my own now at
[[https://github.com/hcpsilva/dotfiles/blob/master/.emacs.d/config.org][this]] link.

*** 2019-05-08

I studied a lot of database fundamentals, as I had it's exam by afternoon.

*** 2019-05-09

I started the day by reading about and learning =tmux=, which is, as it's
called, an "terminal multiplexer". Knowing how to use =tmux= will help me to run
commands and close the =ssh= connection, leaving the session open so I can
easily come back and resume the operations and tasks I was performing.

Also, I read the LLNL's tutorial on Linux clusters and gathered a lot of new
resources to complement my =External resources= section (besides learning a lot,
obviously).

** Meetings

This could stay inside its respective entry in the daily journal, but I think
that separating meetings from the dailies is better.

*** 2019-04-30 Tips for ORG-Mode                                     :ATTACH:
    :PROPERTIES:
    :ID:       428b174d-ec00-474e-b65c-cc8671da1019
    :END:

See the attached file in [[./attachments/init.org]], or follow the update
instructions [[http://mescal.imag.fr/membres/arnaud.legrand/misc/init.php][here]] that points to the [[https://app-learninglab.inria.fr/gitlab/learning-lab/mooc-rr-ressources/blob/master/module2/ressources/emacs_orgmode.org][learninglab]].

* External resources

Here I'll categorize useful resources I've found while "aggressively" googling
and/or reading papers and other documents.

** Linux

Any useful Linux knowledge relevant to my activities should stay here.

*** tmux

#+BEGIN_QUOTE
=tmux= is a terminal multiplexer for Unix-like operating systems. It allows
multiple terminal sessions to be accessed simultaneously in a single window. It
is useful for running more than one command-line program at the same time. It
can also be used to detach processes from their controlling terminals, allowing
SSH sessions to remain active without being visible.
#+END_QUOTE

Tutorials:

- [[https://edricteo.com/tmux-tutorial/]]
- [[https://hackernoon.com/a-gentle-introduction-to-tmux-8d784c404340]]
- [[https://danielmiessler.com/study/tmux/]]

** Servers

Here lies all knowledge I don't possess about servers and cluster and so on and
so forth.

*** Clusters

- [[https://whatis.suse.com/definition/linux-cluster/][Definition]]

*Tutorials*:

- [[https://www.ibm.com/developerworks/aix/tutorials/clustering/clustering.html][IBM]]
  From 2002 but still explains a lot of the fundamental concepts.
- [[https://computing.llnl.gov/tutorials/linux_clusters/][LLNL]]
  Huge! Includes exercises, Slurm, GPU clusters, and much more.
- [[https://www.wikiwand.com/en/Computer_cluster][Wikipedia]]
  Explains pretty well in layman terms what is a cluster.

*** Slurm

#+BEGIN_QUOTE
Slurm is an open source, fault-tolerant, and highly scalable cluster management
and job scheduling system for large and small Linux clusters.
#+END_QUOTE

- [[https://slurm.schedmd.com/documentation.html][Documentation]]

Tutorials:

- [[https://slurm.schedmd.com/tutorials.html][Documentation tutorial]]
- [[https://computing.llnl.gov/tutorials/moab/][LLNL's tutorial]]

**** Useful commands:

 - =sacct= :: is used to report job or job step accounting information about active
            or completed jobs.

 - =salloc= :: is used to allocate resources for a job in real time. Typically this
             is used to allocate resources and spawn a shell.

 - =sattach= :: is used to attach standard input, output, and error plus signal
              capabilities to a currently running job or job step. One can attach
              to and detach from jobs multiple times.

 - =sbatch= :: is used to submit a job script for later execution. The script will
             typically contain one or more srun commands to launch parallel tasks.

 - =sbcast= :: is used to transfer a file from local disk to local disk on the
             nodes allocated to a job.

 - =scancel= :: is used to cancel a pending or running job or job step. It can also
              be used to send an arbitrary signal to all processes associated
              with a running job or job step.

 - =sinfo= :: reports the state of partitions and nodes managed by Slurm. It has a
            wide variety of filtering, sorting, and formatting options.

 - =smap= :: reports state information for jobs, partitions, and nodes managed by
           Slurm, but graphically displays the information to reflect network
           topology.

 - =squeue= :: reports the state of jobs or job steps. By default, it reports the
             running jobs in priority order and then the pending jobs in priority
             order.

 - =srun= :: is used to submit a job for execution or initiate job steps in real
           time.

 - =strigger= :: is used to set, get or view event triggers. Event triggers
               include things such as nodes going down or jobs approaching their
               time limit.

 - =sview= :: is a graphical user interface to get and update state information for
            jobs, partitions, and nodes managed by Slurm.

 All command's manuals are in =man=, so no worries if this is to little info.

*** Spack

#+BEGIN_QUOTE
Spack is a package management tool designed to support multiple versions and
configurations of software on a wide variety of platforms and environments. It
was designed for large supercomputing centers, where many users and application
teams share common installations of software on clusters with exotic
architectures, using libraries that do not have a standard ABI.
#+END_QUOTE

- [[https://github.com/spack/spack][GitHub page]]
- [[https://spack.readthedocs.io/en/latest/][Documentation]]
  - [[https://spack.readthedocs.io/en/latest/tutorial.html][Tutorial]]

*** PCAD

The GPPD manages the High Performance Computation Park (PCAD) and is the group
I'm part of!

- [[http://gppd-hpc.inf.ufrgs.br/][Presentation]]

** Programming

Here lies all programming and HPC-related knowledge.

*** MPI

#+BEGIN_QUOTE
Message Passing Interface (MPI) is a standardized and portable message-passing
standard designed by a group of researchers from academia and industry to
function on a wide variety of parallel computing architectures.
#+END_QUOTE

- [[https://www.wikiwand.com/en/Message_Passing_Interface][Wikipedia]]
- [[https://computing.llnl.gov/tutorials/mpi/][LLNL's Tutorial]]

**** C++ wrappers

I've gathered some info about MPI wrappers for C++ (because I like both
simplicity and C++).

- [[https://blogs.cisco.com/performance/the-mpi-c-bindings-what-happened-and-why][2012 state of affairs]]

Examples:

- [[https://github.com/boostorg/mpi][boost.mpi]]
- [[https://github.com/patflick/mxx][mxx]]

So it seems to me that either the community has no interest in bindings and
simplicity or things move really slowly when it comes to standards proposed by
scholars and academics.

*** CUDA

#+BEGIN_QUOTE
CUDA is a parallel computing platform and application programming interface
(API) model created by Nvidia.It allows software developers and software
engineers to use a CUDA-enabled graphics processing unit (GPU) for general
purpose processing â€” an approach termed GPGPU (General-Purpose computing on
Graphics Processing Units).
#+END_QUOTE

Tutorials:

- [[https://computing.llnl.gov/tutorials/linux_clusters/gpu/NVIDIA.Introduction_to_CUDA_C.1.pdf][NVIDIA slides]]
- [[http://people.maths.ox.ac.uk/~gilesm/cuda/][Oxford course]]
- [[https://computing.llnl.gov/tutorials/openMP/][LLNL's tutorial]]

*** StarPU

** Research methodology

* Project schedule

Here is the intended project schedule to me:

| Activity                  | May | June | July |
|---------------------------+-----+------+------|
| State of the art / StarPU | x   | x    |      |
| Experimentation           | x   | x    |      |
| Performance analysis      |     | x    | x    |
| Report writing            |     |      | x    |
