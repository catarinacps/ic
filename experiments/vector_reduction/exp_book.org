#+title: Vector Reduction Experiments book
#+author: Henrique Silva
#+email: hcpsilva@inf.ufrgs.br
#+infojs_opt:
#+property: session *R*
#+property: cache yes
#+property: results graphics
#+property: exports both
#+property: tangle yes

It reduces a vector, that's it.

Preliminary library loading:

#+begin_src R :session :results none
library(DoE.base)
options(crayon.enabled = FALSE)
library(tidyverse)
options(scipen = 999)
#+end_src

And the partitions intended to be used in this test:

#+name: machines
#+begin_src bash :tangle no :results output :exports results
echo "draco tupi"
#+end_src

And the scheduling of experiments is supposed to happen according to this
script (which is, by the way, surprisingly generic):

#+begin_src bash :shebang "#!/bin/bash" :tangle launch.sh :var PARTITIONS=machines :results none
# the experiment id
EXPERIMENT_ID=$1

# the work (repo) dir
REPO_DIR=$2

if [[ $REPO_DIR != /* ]]; then
    echo "Path to repository is not absolute, please use the absolute path..."
    exit
fi

EXP_DIR=$REPO_DIR/experiments/vector_reduction/$EXPERIMENT_ID
pushd $REPO_DIR

# always update and overwrite the code dir
git pull
rm -rf $EXP_DIR/code
cp -r code/starpu/vector-reduction $EXP_DIR/code

for name in $PARTITIONS; do
    nodes=$(gppd-info --long --Node -S NODELIST -p $name -h | awk '{print $1 "_" $5}' | paste -s -d" " -)

    for execution in $nodes; do
        # launch the slurm script for this node
        sbatch \
            -p ${name} \
            -w ${execution%%_*} \
            -c ${execution#*_} \
            -J vector_reduction_${EXPERIMENT_ID} \
            $EXP_DIR/exp.slurm $EXPERIMENT_ID $EXP_DIR
    done
done

popd
#+end_src

Which is to be used in the following way:

#+begin_src bash :tangle no
chmod +x launch.sh

./lauch.sh <experiment_id> <path_to_repo>
#+end_src

The maximum value for each element will be:

#+name: max_val
#+begin_src bash :tangle no :results value :exports results
echo 100
#+end_src

#+RESULTS: max_val
: 100

* Table of contents                                                   :TOC_3:
- [[#1---the-basics][1 - The basics]]
  - [[#design][Design]]
  - [[#script][Script]]
  - [[#data-analysis][Data analysis]]
- [[#2---comparing-extra-implementations][2 - Comparing extra implementations]]
  - [[#design-1][Design]]
  - [[#script-1][Script]]
  - [[#data-analysis-1][Data analysis]]

* 1 - The basics                                                      :EXP01:

It exists so I learn how to do stuff using R and slurm and shell scripts.

** Design

The random seed will be:

#+begin_src R :session :results value :exports results
floor(runif(1,1,99999))
#+end_src

#+RESULTS:
: 60485

Now the executions plan:

#+begin_src R :session :results none
size = c(70000000, 300000000, 1100000000)
nb = c(7000, 25000, 82000)
fr = c(2, 10, 1000)

fac.design(
    nfactors=3,
    replications=30,
    repeat.only=FALSE,
    blocks=1,
    randomize=TRUE,
    seed=60485,
    factor.names=list(
      vec_size=size,
      num_blocks=nb,
      reduction_factor=fr)) %>%
  as_tibble %>%
  transmute(id=as.numeric(Blocks), vec_size, num_blocks, reduction_factor) %>%
  write_delim("exp01/runs.plan", delim=" ", col_names=FALSE)

# the space delimited file is to help with the posterior parsing in the shell
# script
#+end_src


** Script

#+begin_src bash :shebang "#!/bin/bash" :tangle exp01/exp.slurm
#SBATCH --time=72:00:00
#SBATCH --workdir=.
#SBATCH --output=/home/users/hcpsilva/slurm_outputs/%x_%j.out
#SBATCH --error=/home/users/hcpsilva/slurm_outputs/%x_%j.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=hcpsilva@inf.ufrgs.br

# parameters
# the experiment ID, defined in the lab-book
EXP_ID=$1
# the experiment directory
EXP_DIR=$2

# as defined in the experiment design
MAX_VALUE=100

# set the hosname as for some god forsaken reason it isn't
HOST=$(hostname)

# experiment name (which is the ID and the machine and its core count)
EXP_NAME=${EXP_ID}_${HOST}_${SLURM_CPUS_ON_NODE}

# go to the scratch dir to execute our operations
cd $SCRATCH

# clean up my scratch dir
rm -rf $SCRATCH/*

# install spack
git clone https://github.com/spack/spack.git
# source spack env variables
. spack/share/spack/setup-env.sh
# add the solverstack INRIA repo
git clone https://gitlab.inria.fr/solverstack/spack-repo.git solverstack
# and add it to spack
spack repo add solverstack

# install starpu and cia
spack install starpu@develop+fxt+poti~examples~mpi~openmp
echo "StarPU installed!"
spack install intel-tbb
echo "TBB intalled!"

# create install dir and put StarPU in it
mkdir install
mkdir install/starpu
mkdir install/tbb
spack view soft install/starpu starpu
spack view soft install/tbb intel-tbb

STARPU_PATH=$(readlink -f install/starpu)
TBB_PATH=$(readlink -f install/tbb)

# set up path and ld path
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$STARPU_PATH/lib:$TBB_PATH/lib
export PATH=$PATH:$STARPU_PATH/bin
export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$STARPU_PATH/lib/pkgconfig

# prepare our directory
mkdir $EXP_NAME
pushd $EXP_NAME

# copy the code folder
cp -r $EXP_DIR/code code
mkdir results

# compile
pushd code
make LIB_EXTRA="-L$TBB_PATH/lib" INC_EXTRA="-I$TBB_PATH/include"
popd

# init the results csv
results_csv=results/${HOST}_data.csv
echo "node,rep_id,vector_size,num_blocks,reduc_fac,compute_time" > $results_csv

# execute the program
while read -r id vector_size num_blocks reduc_fac; do
    echo "-> Parameters set to: $vector_size $num_blocks $reduc_fac"

    # execute with given configurations
    c_time=$(./code/build/starpu $vector_size $num_blocks $reduc_fac $MAX_VALUE)

    # add execution data to csv
    echo ${HOST},${id},${vector_size},${num_blocks},${reduc_fac},${c_time} >> $results_csv

    # stress the memory to prevent cache influence between runs
    stress-ng --vm 3 --vm-bytes 75% -t 5s &> /dev/null

    echo
done < $EXP_DIR/runs.plan

# zip everything and commit to EXP_DIR
tar czf $EXP_DIR/${EXP_NAME}_data.tar.gz *

popd
rm -rf $SCRATCH/*
#+end_src

** Data analysis

Let's first start by getting only the CSVs

#+begin_src shell :results output
#!/usr/bin/bash

cd exp01

for zip in *.tar.gz; do
    temp_dir=${zip%%.*}

    mkdir $temp_dir
    tar xzf $zip -C $temp_dir

    host=$(awk -F "_" '{print $2}' <<$temp_dir)

    mkdir data/$host
    mv $temp_dir/results/*.csv data/$host

    rm -rf $temp_dir
done
#+end_src

#+RESULTS:

Read the data

#+begin_src R :results output :session :exports both
df <- read_csv("exp01/data/draco1_data.csv") %>%
    print
#+end_src

#+RESULTS:
#+begin_example

Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  vector_size = col_double(),
  num_blocks = col_double(),
  reduc_fac = col_double(),
  compute_time = col_double()
)
# A tibble: 810 x 6
   node   rep_id vector_size num_blocks reduc_fac compute_time
   <chr>   <dbl>       <dbl>      <dbl>     <dbl>        <dbl>
 1 draco1      1    70000000      82000      1000        20.1 
 2 draco1      1   300000000      82000      1000        20.2 
 3 draco1      1  1100000000       7000        10         6.05
 4 draco1      1   300000000       7000         2         6.01
 5 draco1      1    70000000      25000        10         5.84
 6 draco1      1  1100000000      82000         2        38.0 
 7 draco1      1  1100000000      25000         2        11.6 
 8 draco1      1    70000000       7000      1000         1.82
 9 draco1      1   300000000      25000         2        11.8 
10 draco1      1    70000000      82000        10        18.5 
# â€¦ with 800 more rows
#+end_example

Now, let's do the shapiro test to check normality:

Are the times conformant to a normal distribution?

#+begin_src R :results output :session :exports both
df %>%
    group_by(vector_size, num_blocks, reduc_fac) %>%
    summarise(N=n(), MeanTime = mean(compute_time), p_value = shapiro.test(compute_time)$p.value) %>%
    arrange(p_value) %>%
    as.data.frame
#+end_src

#+RESULTS:
#+begin_example

   vector_size num_blocks reduc_fac  N  MeanTime     p_value
1   1100000000       7000      1000 30  4.594647 0.003164781
2    300000000       7000        10 30  3.890675 0.003773316
3    300000000       7000         2 30  5.700543 0.013497913
4    300000000       7000      1000 30  3.522460 0.013761107
5   1100000000       7000         2 30  5.841058 0.017641414
6   1100000000       7000        10 30  4.765092 0.018148999
7     70000000       7000      1000 30  1.303133 0.022394600
8     70000000      25000         2 30 13.615184 0.028352232
9    300000000      25000      1000 30  7.612575 0.029788209
10   300000000      25000        10 30  8.539796 0.036558323
11  1100000000      25000        10 30  7.906801 0.060296884
12  1100000000      25000      1000 30  6.450960 0.097830024
13  1100000000      25000         2 30 13.940253 0.113799315
14  1100000000      82000         2 30 39.021747 0.129786305
15   300000000      25000         2 30 15.432226 0.195135678
16   300000000      82000        10 30 21.560584 0.210911456
17   300000000      82000         2 30 38.404493 0.211928470
18    70000000      82000         2 30 40.101594 0.295462727
19    70000000       7000        10 30  1.580933 0.397691217
20    70000000       7000         2 30  3.496167 0.479513826
21  1100000000      82000      1000 30 17.183202 0.686061637
22    70000000      82000        10 30 20.199258 0.752221337
23    70000000      25000        10 30  6.233050 0.805414325
24    70000000      82000      1000 30 17.517267 0.819914985
25  1100000000      82000        10 30 19.980348 0.823937994
26    70000000      25000      1000 30  5.080383 0.893796703
27   300000000      82000      1000 30 18.467808 0.923618902
#+end_example

Conclusion
- If it is greater than 0.05, then it is normal.
- Let's that 30% may be considered not normal

Let's do the histogram.

- for 1000

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session
df %>%
    filter(reduc_fac == 1000) %>%
    ggplot(aes(x=compute_time)) +
    geom_histogram(binwidth=0.5) +
    facet_grid(vector_size~num_blocks)
#+end_src

#+RESULTS:
[[file:/tmp/babel-sj50zX/figurejiClgf.png]]

- for 10

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session
df %>%
    filter(reduc_fac == 10) %>%
    ggplot(aes(x=compute_time)) +
    geom_histogram(binwidth=0.5) +
    facet_grid(vector_size~num_blocks)
#+end_src

#+RESULTS:
[[file:/tmp/babel-sj50zX/figureng9nmD.png]]


- for 2

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session
df %>%
    filter(reduc_fac == 2) %>%
    ggplot(aes(x=compute_time)) +
    geom_histogram(binwidth=0.5) +
    facet_grid(vector_size~num_blocks)
#+end_src

#+RESULTS:
[[file:/tmp/babel-sj50zX/figureh84kLH.png]]


#+begin_src R :results output :session :exports both
df %>%
    group_by(vector_size, num_blocks, reduc_fac) %>%
    summarise(N=n(),
              MeanTime = mean(compute_time),
              SE = 3*sd(compute_time)/sqrt(N)) %>%
    as.data.frame
#+end_src

#+RESULTS:
#+begin_example

   vector_size num_blocks reduc_fac  N  MeanTime         SE
1     70000000       7000         2 30  3.496167 0.14004833
2     70000000       7000        10 30  1.580933 0.08507475
3     70000000       7000      1000 30  1.303133 0.12848962
4     70000000      25000         2 30 13.615184 0.80895838
5     70000000      25000        10 30  6.233050 0.37859280
6     70000000      25000      1000 30  5.080383 0.22732792
7     70000000      82000         2 30 40.101594 1.68835458
8     70000000      82000        10 30 20.199258 1.16571309
9     70000000      82000      1000 30 17.517267 1.01353185
10   300000000       7000         2 30  5.700543 0.88157222
11   300000000       7000        10 30  3.890675 0.55050488
12   300000000       7000      1000 30  3.522460 0.60607674
13   300000000      25000         2 30 15.432226 1.30460550
14   300000000      25000        10 30  8.539796 0.98848439
15   300000000      25000      1000 30  7.612575 0.89199800
16   300000000      82000         2 30 38.404493 1.91787513
17   300000000      82000        10 30 21.560584 1.47803147
18   300000000      82000      1000 30 18.467808 1.42089492
19  1100000000       7000         2 30  5.841058 1.39846391
20  1100000000       7000        10 30  4.765092 1.34070884
21  1100000000       7000      1000 30  4.594647 1.26545327
22  1100000000      25000         2 30 13.940253 1.52019790
23  1100000000      25000        10 30  7.906801 1.33685862
24  1100000000      25000      1000 30  6.450960 1.40355702
25  1100000000      82000         2 30 39.021747 1.68049247
26  1100000000      82000        10 30 19.980348 1.37349250
27  1100000000      82000      1000 30 17.183202 1.50195493
#+end_example

Do a plot
- with dots and 

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session
df %>%
    group_by(vector_size, num_blocks, reduc_fac) %>%
    summarise(N=n(),
              MeanTime = mean(compute_time),
              SE = 3*sd(compute_time)/sqrt(N)) %>%
    ungroup %>%
    mutate(Reduction = as.factor(reduc_fac),
           NBlocks = as.factor(num_blocks)) %>%
    mutate(vector_size = vector_size/10000000) %>%
    ggplot(aes(x=as.factor(vector_size),
               y=MeanTime,
               #color=Reduction,
               color=NBlocks)) +
    ylim(0,NA) +
    theme_bw(base_size=16) +
    xlab("Vector Size [x10^7]") +
    ylab("Average Compute Time [s]") +
    geom_point(size=3, position=position_dodge(width = .25)) + #position = position_dodge2(preserve = "single")) + #position = "dodge") +
    geom_errorbar(aes(ymin=MeanTime-SE, ymax=MeanTime+SE), width=.5, position=position_dodge(width = .25)) +
    theme(legend.position = "top") +
    facet_wrap(~Reduction)
#+end_src

#+RESULTS:
[[file:/tmp/babel-sj50zX/figureH4UVni.png]]


* 2 - Comparing extra implementations                                 :EXP02:

Because it's never enough.

** Design

The random seed will be:

#+begin_src R :session :results value :exports results
floor(runif(1,1,99999))
#+end_src

#+RESULTS:
: 95099

And the execution plan (same sizes as before):

#+begin_src R :session :results none
size = c(70000000, 300000000, 1100000000)
ver= c("naive", "accumulate", "openmp")

fac.design(
    nfactors=2,
    replications=30,
    repeat.only=FALSE,
    blocks=1,
    randomize=TRUE,
    seed=95099,
    factor.names=list(
      vec_size=size,
      version=ver)) %>%
  as_tibble %>%
  transmute(id=as.numeric(Blocks), version, vec_size) %>%
  write_delim("exp02/runs.plan", delim=" ", col_names=FALSE)

# the space delimited file is to help with the posterior parsing in the shell
# script
#+end_src

** Script

#+begin_src bash :shebang "#!/bin/bash" :tangle exp02/exp.slurm
#SBATCH --time=72:00:00
#SBATCH --workdir=.
#SBATCH --output=/home/users/hcpsilva/slurm_outputs/%x_%j.out
#SBATCH --error=/home/users/hcpsilva/slurm_outputs/%x_%j.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=hcpsilva@inf.ufrgs.br

# parameters
# the experiment ID, defined in the lab-book
EXP_ID=$1
# the experiment directory
EXP_DIR=$2

# as defined in the experiment design
MAX_VALUE=100

# set the hosname as for some god forsaken reason it isn't
HOST=$(hostname)

# experiment name (which is the ID and the machine and its core count)
EXP_NAME=${EXP_ID}_${HOST}_${SLURM_CPUS_ON_NODE}

# go to the scratch dir to execute our operations
cd $SCRATCH

# clean up my scratch dir
rm -rf $SCRATCH/*

# install spack
git clone https://github.com/spack/spack.git
# source spack env variables
. spack/share/spack/setup-env.sh
# add the solverstack INRIA repo
git clone https://gitlab.inria.fr/solverstack/spack-repo.git solverstack
# and add it to spack
spack repo add solverstack

# install starpu and cia
spack install starpu@develop+fxt+poti~examples~mpi~openmp
echo "StarPU installed!"
spack install intel-tbb
echo "TBB intalled!"

# create install dir and put StarPU in it
mkdir install
mkdir install/starpu
mkdir install/tbb
spack view soft install/starpu starpu
spack view soft install/tbb intel-tbb

STARPU_PATH=$(readlink -f install/starpu)
TBB_PATH=$(readlink -f install/tbb)

# set up path and ld path
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$STARPU_PATH/lib:$TBB_PATH/lib
export PATH=$PATH:$STARPU_PATH/bin
export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$STARPU_PATH/lib/pkgconfig

# prepare our directory
mkdir $EXP_NAME
pushd $EXP_NAME

# copy the code folder
cp -r $EXP_DIR/code code
mkdir results

# compile
pushd code
make LIB_EXTRA="-L$TBB_PATH/lib" INC_EXTRA="-I$TBB_PATH/include"
popd

# init the results csv
results_csv=results/${HOST}_data.csv
echo "node,rep_id,version,vector_size,compute_time" > $results_csv

# execute the program
while read -r id version vector_size; do
    echo "-> Parameters set to: $version $vector_size"

    # execute with given configurations
    c_time=$(./code/build/$version $vector_size $MAX_VALUE)

    # add execution data to csv
    echo ${HOST},${id},${version},${vector_size},${c_time} >> $results_csv

    # stress the memory to prevent cache influence between runs
    stress-ng --vm 3 --vm-bytes 75% -t 5s &> /dev/null

    echo
done < $EXP_DIR/runs.plan

# zip everything and commit to EXP_DIR
tar czf $EXP_DIR/${EXP_NAME}_data.tar.gz *

popd
rm -rf $SCRATCH/*
#+end_src

** Data analysis

Let's first start by getting only the CSVs

#+begin_src shell :results output
cd exp02

mkdir data

for zip in *.tar.gz; do
    temp_dir=${zip%%.*}

    mkdir $temp_dir
    tar xzf $zip -C $temp_dir

    host=$(awk '{print $2}' <<<$temp_dir)

    mv $temp_dir/results/*.csv data

    rm -rf $temp_dir
done
#+end_src

#+RESULTS:

Read the data

#+begin_src R :results output :session :exports both
df <- read_csv("exp02/data/draco1_data.csv") %>%
    print
#+end_src

#+RESULTS:
#+begin_example

Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  version = col_character(),
  vector_size = col_double(),
  compute_time = col_double()
)
# A tibble: 270 x 5
   node   rep_id version    vector_size compute_time
   <chr>   <dbl> <chr>            <dbl>        <dbl>
 1 draco1      1 naive         70000000       0.0568
 2 draco1      1 naive        300000000       0.242 
 3 draco1      1 openmp       300000000       0.162 
 4 draco1      1 accumulate    70000000       0.0576
 5 draco1      1 accumulate   300000000       0.245 
 6 draco1      1 naive       1100000000       0.895 
 7 draco1      1 openmp        70000000       0.0412
 8 draco1      1 accumulate  1100000000       0.924 
 9 draco1      1 openmp      1100000000       0.583 
10 draco1     12 naive         70000000       0.0569
# â€¦ with 260 more rows
#+end_example

#+begin_src R :results output :session :exports both
df %>%
    group_by(node, version, vector_size) %>%
    summarise(N=n(),
              MeanTime = mean(compute_time),
              SE = 3*sd(compute_time)/sqrt(N)) %>%
    ungroup %>%
    as.data.frame
#+end_src

#+RESULTS:
#+begin_example

    node    version vector_size  N   MeanTime           SE
1 draco1 accumulate    70000000 30 0.05804833 0.0004338173
2 draco1 accumulate   300000000 30 0.24678100 0.0017265984
3 draco1 accumulate  1100000000 30 0.91754033 0.0026951184
4 draco1      naive    70000000 30 0.05760000 0.0006570246
5 draco1      naive   300000000 30 0.24257000 0.0004419627
6 draco1      naive  1100000000 30 0.89073467 0.0015300999
7 draco1     openmp    70000000 30 0.04260167 0.0012368063
8 draco1     openmp   300000000 30 0.16297967 0.0014036441
9 draco1     openmp  1100000000 30 0.58503367 0.0021530878
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session
df %>%
    group_by(node, version, vector_size) %>%
    summarise(N=n(),
              MeanTime = mean(compute_time),
              SE = 3*sd(compute_time)/sqrt(N)) %>%
    ungroup %>%
    mutate(vector_size = vector_size/10000000) %>%
    ggplot(aes(x=as.factor(vector_size),
               y=MeanTime,
               color=version)) +
    ylim(0,NA) +
    theme_bw(base_size=16) +
    xlab("Vector Size [x10^7]") +
    ylab("Average Compute Time [s]") +
    geom_point(size=3, position=position_dodge(width = .25)) + #position = position_dodge2(preserve = "single")) + #position = "dodge") +
    geom_errorbar(aes(ymin=MeanTime-SE, ymax=MeanTime+SE), width=.5, position=position_dodge(width = .25)) +
    theme(legend.position = "top")
#+end_src

#+RESULTS:
[[file:/tmp/babel-sj50zX/figurebUmnac.png]]

Let's mix exp01 and exp02

#+begin_src R :results output :session :exports both
df.exp1 <- read_csv("exp01/data/draco1_data.csv") %>%
    filter(num_blocks == 7000) %>%
    filter(reduc_fac == 1000) %>%
    group_by(node, vector_size, num_blocks, reduc_fac) %>%
    summarise(N=n(),
              MeanTime = mean(compute_time),
              SE = 3*sd(compute_time)/sqrt(N)) %>%
    ungroup %>%
    mutate(Reduction = as.factor(reduc_fac),
           NBlocks = as.factor(num_blocks)) %>%
    mutate(vector_size = vector_size/10000000) %>%
    select(-contains("educ"), -contains("locks")) %>%
    mutate(version = "starpu") %>%
    print

df.exp2 <- read_csv("exp02/data/draco1_data.csv") %>%
    group_by(node, version, vector_size) %>%
    summarise(N=n(),
              MeanTime = mean(compute_time),
              SE = 3*sd(compute_time)/sqrt(N)) %>%
    mutate(vector_size = vector_size/10000000) %>%
    print

df.exp1exp2 <- df.exp1 %>%
    bind_rows(df.exp2) %>%
    print
#+end_src

#+RESULTS:
#+begin_example

Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  vector_size = col_double(),
  num_blocks = col_double(),
  reduc_fac = col_double(),
  compute_time = col_double()
)
# A tibble: 3 x 6
  node   vector_size     N MeanTime    SE version
  <chr>        <dbl> <int>    <dbl> <dbl> <chr>  
1 draco1           7    30     1.30 0.128 starpu 
2 draco1          30    30     3.52 0.606 starpu 
3 draco1         110    30     4.59 1.27  starpu

Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  version = col_character(),
  vector_size = col_double(),
  compute_time = col_double()
)
# A tibble: 9 x 6
# Groups:   node, version [3]
  node   version    vector_size     N MeanTime       SE
  <chr>  <chr>            <dbl> <int>    <dbl>    <dbl>
1 draco1 accumulate           7    30   0.0580 0.000434
2 draco1 accumulate          30    30   0.247  0.00173 
3 draco1 accumulate         110    30   0.918  0.00270 
4 draco1 naive                7    30   0.0576 0.000657
5 draco1 naive               30    30   0.243  0.000442
6 draco1 naive              110    30   0.891  0.00153 
7 draco1 openmp               7    30   0.0426 0.00124 
8 draco1 openmp              30    30   0.163  0.00140 
9 draco1 openmp             110    30   0.585  0.00215

# A tibble: 12 x 6
   node   vector_size     N MeanTime       SE version   
   <chr>        <dbl> <int>    <dbl>    <dbl> <chr>     
 1 draco1           7    30   1.30   0.128    starpu    
 2 draco1          30    30   3.52   0.606    starpu    
 3 draco1         110    30   4.59   1.27     starpu    
 4 draco1           7    30   0.0580 0.000434 accumulate
 5 draco1          30    30   0.247  0.00173  accumulate
 6 draco1         110    30   0.918  0.00270  accumulate
 7 draco1           7    30   0.0576 0.000657 naive     
 8 draco1          30    30   0.243  0.000442 naive     
 9 draco1         110    30   0.891  0.00153  naive     
10 draco1           7    30   0.0426 0.00124  openmp    
11 draco1          30    30   0.163  0.00140  openmp    
12 draco1         110    30   0.585  0.00215  openmp
#+end_example

Let's plot my final sadness

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session
df.exp1exp2 %>%
    ggplot(aes(x=as.factor(vector_size),
               y=MeanTime,
               color=version)) +
    ylim(0,NA) +
    theme_bw(base_size=16) +
    xlab("Vector Size [x10^7]") +
    ylab("Average Compute Time [s]") +
    geom_point(size=3, position=position_dodge(width = .25)) +
    geom_errorbar(aes(ymin=MeanTime-SE, ymax=MeanTime+SE), width=.5, position=position_dodge(width = .25)) +
    theme(legend.position = "top")
#+end_src

#+RESULTS:
[[file:/tmp/babel-sj50zX/figure4CrkPU.png]]
