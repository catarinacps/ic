#+title: Vector Reduction Experiments book
#+author: Henrique Silva
#+email: hcpsilva@inf.ufrgs.br
#+infojs_opt:
#+property: session *R*
#+property: cache yes
#+property: results graphics
#+property: exports both
#+property: tangle yes

It reduces a vector, that's it.

Preliminary library loading:

#+begin_src R :session :results none
library(DoE.base)
library(tidyverse)
#+end_src

And the partitions intended to be used in this test:

#+name: machines
#+begin_src bash :tangle no
echo "draco hype tupi"
#+end_src

And the scheduling of experiments is supposed to happen according to this
script (which is, by the way, surprisingly generic):

#+begin_src bash :shebang "#!/bin/bash" :tangle launch.sh :var PARTITIONS=machines :results none
# the experiment id
EXPERIMENT_ID=$1

# the work (repo) dir
REPO_DIR=$2

if [[ $REPO_DIR != /* ]]; then
    echo "Path to repository is not absolute, please use the absolute path..."
    exit
fi

EXP_DIR=$REPO_DIR/experiments/vector_reduction/$EXPERIMENT_ID
pushd $REPO_DIR

# always update and overwrite the code dir
git pull
cp -r code $EXP_DIR/code

for name in $PARTITIONS; do
    nodes=$(gppd-info --long --Node -S NODELIST -p $name -h | awk '{print $1 "_" $5}' | paste -s -d" " -)

    for execution in $nodes; do
        # launch the slurm script for this node
        sbatch \
            -w ${execution%%_*} \
            -c ${execution#*_} \
            -J ${EXPERIMENT_ID}_${execution}_${USER} \
            $EXPERIMENT_ID/exp.slurm $EXPERIMENT_ID $EXP_DIR
    done
done

popd
#+end_src

Which is to be used in the following way:

#+begin_src bash :tangle no
chmod +x launch.sh

./lauch.sh <experiment_id> <path_to_repo>
#+end_src

* Table of contents                                                   :TOC_3:
- [[#1---the-basics][1 - The basics]]
  - [[#design][Design]]
  - [[#script][Script]]

* 1 - The basics                                                      :EXP01:

It exists so I learn how to do stuff using R and slurm and shell scripts.

** Design

The random seed will be:

#+begin_src R :session :results value :exports results
floor(runif(1,1,99999))
#+end_src

#+RESULTS:
: 60485

#+begin_src R :session :results none
size = c(70000000, 300000000, 1100000000)
nb = c(7000, 25000, 82000)
fr = c(2, 10, 1000)

fac.design(
    nfactors=3,
    replications=10,
    repeat.only=FALSE,
    blocks=1,
    randomize=TRUE,
    seed=60485,
    factor.names=list(
        vec_size=size,
        num_blocks=nb,
        reduction_factor=fr)) %>%
  as_tibble %>%
  transmute(id=as.numeric(Blocks), vec_size, num_blocks, reduction_factor) %>%
  write_delim("exp01/runs.plan", delim=" ", col_names=FALSE)

# the space delimited file is to help with the posterior parsing in the shell
# script
#+end_src

** Script

#+begin_src bash :shebang "#!/bin/bash" :tangle exp01/exp.slurm
#SBATCH --time=72:00:00
#SBATCH --workdir=.
#SBATCH --output=/home/users/hcpsilva/slurm_outputs/%x_%j.out
#SBATCH --error=/home/users/hcpsilva/slurm_outputs/%x_%j.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=hcpsilva@inf.ufrgs.br

# parameters
# the experiment ID, defined in the lab-book
EXP_ID=$1
# the experiment directory
EXP_DIR=$2

# experiment name (which is the ID and the machine and its core count)
EXP_NAME=${EXP_ID}_${HOST}_${SLURM_CPUS_ON_NODE}

# seed generated in project design
RAND_SEED=60485

# go to the scratch dir to execute our operations
cd $SCRATCH

# clean up my scratch dir
rm -rf *

# install spack
git clone https://github.com/spack/spack.git
# source spack env variables
. spack/share/spack/setup-env.sh

# install starpu and cia
spack install starpu@develop+fxt+poti~examples~mpi~openmp &> /dev/null
echo "StarPU installed!"

# set up path and ld path
export LD_LIBRARY_PATH=$(spack location -i starpu)/lib
export PATH=$PATH:$(spack location -i starpu)/bin

# prepare our directory
mkdir $EXP_NAME
pushd $EXP_NAME

# copy the code folder
cp -r $EXP_DIR/code code
mkdir results

# compile
pushd code
[ ! -d build/ ] && mkdir build
pushd build
cmake -DSTARPU_DIR=$(spack location -i starpu) ..
cmake -DSTARPU_DIR=$(spack location -i starpu) ..
make
popd
popd

# init the results csv
results_csv=results/${HOST}_data.csv
echo "node,rep_id,matrix_size,block_size,runtime,compute_time,total_time" > $results_csv

# execute the program
while read -r id vector_size num_blocks reduc_fac; do
    echo "-> Parameters set to: $vector_size $num_blocks $reduc_fac"

    # output log file
    log_file=results/${vector_size}_${num_blocks}_${reduc_fac}_${id}.log

    # execute with given configurations
    ./code/bin/vector-reduction $vector_size $num_blocks $reduc_fac > $log_file

    # TODO: add the extra tested programs (naive reduction)

    # add execution data to csv
    echo ${HOST},${id},${vector_size},${num_blocks},${reduc_fac} >> $results_csv

    # stress the memory to prevent cache influence between runs
    stress-ng --vm 3 --vm-bytes 75% -t 5s &> /dev/null

    echo
done < $EXP_DIR/runs.plan

# zip everything and commit to EXP_DIR
tar czf $EXP_DIR/${EXP_NAME}_data.tar.gz *

popd
rm -rf $SCRATCH/*
#+end_src
